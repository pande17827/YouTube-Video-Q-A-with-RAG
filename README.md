# ğŸ® YouTube Video Q&A with RAG

<div align="center">
  
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black)
![LangChain](https://img.shields.io/badge/LangChain-00A67E?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

</div>

---

## ğŸ“Œ Table of Contents

- [ğŸŒŸ Features](#-features)
- [ğŸ¥ Demo](#-demo)
- [ğŸš€ Installation](#-installation)
- [âš™ï¸ Usage](#%ef%b8%8f-usage)
- [ğŸª  Configuration](#-configuration)
- [ğŸ§ How It Works](#-how-it-works)
- [ğŸ’» Tech Stack](#-tech-stack)
- [ğŸ¦˜ Support](#-support)
- [ğŸ“œ License](#-license)

---

## ğŸŒŸ Features

- âœ”ï¸ Automatic YouTube transcript extraction
- âœ”ï¸ Customizable Hugging Face models (both generation and embeddings)
- âœ”ï¸ Adjustable text chunking parameters
- âœ”ï¸ FAISS vector store for efficient similarity search
- âœ”ï¸ Beautiful Streamlit UI with dark/light mode
- âœ”ï¸ Multi-language support
- âœ”ï¸ Context-aware question answering

---

## ğŸ¥ Demo
*Main UI*
![app.py](docs/ui.png) 
*Result*
![alt text](docs/result.png)


---

## ğŸš€ Installation

### ğŸ“‹ Prerequisites

- Python 3.8+
- [Hugging Face Account](https://huggingface.co)
- [FFmpeg](https://ffmpeg.org/) (for audio processing if needed)

### ğŸ›† Steps

```bash
# Clone repository
https://github.com/pande17827/YouTube-Video-Q-A-with-RAG.git
cd YouTube-Video-Q-A-with-RAG

# Create virtual environment
python -m venv venv

# Activate environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### ğŸš€ Running the App

```bash
streamlit run app.py
```

---

## ğŸª  Configuration

### ğŸ”§ Model Options

| ğŸ§™ï¸â€â™‚ï¸ Component  | âš™ï¸ Options                                                                 |
|--------------|------------------------------------------------------------------------------|
| ğŸ¤– **LLM**        | Qwen, Mistral, FLAN-T5, BART, GPT-2, or custom                                |
| ğŸ§  **Embeddings** | all-MiniLM-L6-v2, bge-small-en, e5-small-v2, or custom                      |
| ğŸ’ƒâœ¨ **Vector DB**   | FAISS, Chroma, Weaviate, Pinecone, Qdrant                                   |

### ğŸŒ Environment Variables

Create a `.env` file in the root directory with the following content:

```ini
HUGGINGFACEHUB_API_TOKEN=your_hf_token_here

# Optional depending on vector DB choice:
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=your_environment
QDRANT_API_KEY=your_qdrant_key
```

---

## ğŸ“Š Project Structure

```
youtube-qa-rag/
â”œâ”€â”€ app.py                # Main application code
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # This documentation
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ docs/                 # Documentation assets
|   â””â”€â”€ final_result.png
|   â””â”€â”€ Question_Answering.png
â”‚   â”œâ”€â”€ result.png
â”‚   â””â”€â”€ setiing.png
|   â””â”€â”€ Text_preprocessing.png
|   â””â”€â”€ transcript Extraction.png
|   â””â”€â”€ ui.png
â””â”€â”€ .gitignore            # Git ignore rules
```

---

## ğŸŒŸ Features in Detail

### 1. Video Processing Pipeline
ğŸ¥ **Pipeline**  
The project includes a robust video processing pipeline designed to handle everything from ingestion to analysis.
![alt text](<docs/transcript Extraction.png>
)
![alt text](docs/Text_preprocessing.png)
![alt text](docs/Question_Answering.png)


### 2. ğŸ› ï¸ Customizable Components

```python
# Example of switching vector stores
vector_store = create_vector_store(
    chunks,
    embeddings,
    "pinecone",  # or "faiss", "chroma", etc.
    pinecone_api_key=os.getenv("PINECONE_API_KEY"),
    pinecone_env="us-west1-gcp",
    pinecone_index="youtube-videos"
)
```

### 3. Beautiful UI Components

ğŸ–¼ï¸ **UI Highlights**  
- **Video thumbnail preview**: Quickly view video snapshots.  
- **Interactive settings panel**: Customize processing parameters on the fly.  
- **Real-time processing indicators**: Monitor progress as tasks are completed.  
- **Responsive answer display**: Get results in a clean and readable format.

---

## ğŸ¤– Supported Models

### Generation Models

| **Model**           | **Size** | **Best For**             |
|---------------------|----------|--------------------------|
| Qwen/Qwen1.5-32B    | 32B      | General purpose          |
| Mistral-7B          | 7B       | High-quality responses   |
| FLAN-T5 Large       | 3B       | Summarization tasks      |

### Embedding Models

| **Model**           | **Dimensions** | **Speed**   |
|---------------------|----------------|-------------|
| all-MiniLM-L6-v2    | 384            | Fast        |
| bge-small-en        | 384            | Balanced    |
| e5-small-v2         | 384            | Efficient   |

---

## ğŸ—“ï¸ Vector Database Options

| **Database** | **Type**  | **Persistence** | **Best For**            |
|--------------|-----------|-----------------|-------------------------|
| FAISS        | Local     | No              | Quick experiments       |
| Chroma       | Local     | Optional        | Development             |
| Pinecone     | Cloud     | Yes             | Production              |
| Qdrant       | Both      | Yes             | Scalable solutions      |

---

## ğŸ“š How It Works

1. **Transcript Extraction**  
   Gets video captions using the YouTube API.  

2. **Text Processing**  
   Splits the transcript into chunks with configurable sizes.  

3. **Embedding Generation**  
   Creates vector representations of the processed text.  

4. **Vector Storage**  
   Stores the generated vectors in the selected database.  

5. **Question Answering**  
   Retrieves relevant context from the stored data and generates accurate answers.

---

## ğŸ› ï¸ Customization

### Adding New Models

You can easily add new models by updating the predefined models dictionary:

```python
llm_options["New Model"] = "username/model-name"
```

### Using Different Databases

Implement new database connectors in the `create_vector_store()` function to integrate additional vector databases seamlessly.

---

## ğŸ“¸ Screenshots

| **Settings Panel**       | **Answer Display**         |
|--------------------------|----------------------------|
| ![Settings](docs/setting.png) | ![Answer](docs/final_result.png) |
|

---

## ğŸ¤ Contributing

Contributions are what make the open-source community an amazing place to learn and grow. Here's how you can contribute:

1. Fork the project.  
2. Create your feature branch:  
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. Commit your changes:  
   ```bash
   git commit -m 'Add some AmazingFeature'
   ```
4. Push to the branch:  
   ```bash
   git push origin feature/AmazingFeature
   ```
5. Open a Pull Request.

---

## ğŸ“œ License

This project is distributed under the **MIT License**. See the [LICENSE](LICENSE) file for more information.

---

## âœ‰ï¸ Contact

Feel free to reach out with any questions or suggestions:

- **Your Name**: [Vikas Kumar](https://twitter.com/yourtwitter)  
- **Email**: [pande.17827@gmail.com](mailto:your.email@example.com)  
- **Project Link**: https://github.com/pande17827/YouTube-Video-Q-A-with-RAG.git

---

**Note:** To use this README:
1. Replace placeholder URLs with your actual project links
2. Add real screenshots
3. Update contact information
4. Customize any sections to better match your project

